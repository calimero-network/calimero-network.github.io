"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[931],{39785:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"developer-tools/merobox/testing-framework-integration","title":"Testing Framework Integration","description":"Advanced testing configuration, custom utilities, and performance testing with Merobox.","source":"@site/docs/05-developer-tools/merobox/testing-framework-integration.mdx","sourceDirName":"05-developer-tools/merobox","slug":"/developer-tools/merobox/testing-framework-integration","permalink":"/developer-tools/merobox/testing-framework-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/calimero-network/calimero-network.github.io/edit/master/docs/05-developer-tools/merobox/testing-framework-integration.mdx","tags":[],"version":"current","lastUpdatedAt":1757979038000,"sidebarPosition":7.6,"frontMatter":{"title":"Testing Framework Integration","description":"Advanced testing configuration, custom utilities, and performance testing with Merobox.","sidebar_position":7.6},"sidebar":"tutorialSidebar","previous":{"title":"Workflow Advanced Features","permalink":"/developer-tools/merobox/workflow-advanced-features"},"next":{"title":"Resource Management","permalink":"/developer-tools/merobox/resource-management"}}');var r=t(86070),o=t(48854);const i={title:"Testing Framework Integration",description:"Advanced testing configuration, custom utilities, and performance testing with Merobox.",sidebar_position:7.6},a="Testing Framework Integration",l={},c=[{value:"Advanced Testing Configuration",id:"advanced-testing-configuration",level:2},{value:"Basic Testing Setup",id:"basic-testing-setup",level:3},{value:"Advanced Cluster Configuration",id:"advanced-cluster-configuration",level:3},{value:"Multi-Environment Testing",id:"multi-environment-testing",level:3},{value:"Custom Test Utilities",id:"custom-test-utilities",level:2},{value:"Basic Test Utilities",id:"basic-test-utilities",level:3},{value:"Advanced Test Utilities",id:"advanced-test-utilities",level:3},{value:"Test Data Management",id:"test-data-management",level:3},{value:"Performance Testing",id:"performance-testing",level:2},{value:"Basic Performance Test",id:"basic-performance-test",level:3},{value:"Advanced Performance Testing",id:"advanced-performance-testing",level:3},{value:"Stress Testing",id:"stress-testing",level:3},{value:"Integration with Testing Frameworks",id:"integration-with-testing-frameworks",level:2},{value:"Pytest Integration",id:"pytest-integration",level:3},{value:"Test Cases",id:"test-cases",level:3},{value:"Continuous Integration",id:"continuous-integration",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Test Organization",id:"test-organization",level:3},{value:"Performance Testing",id:"performance-testing-1",level:3},{value:"Error Handling",id:"error-handling",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"testing-framework-integration",children:"Testing Framework Integration"})}),"\n",(0,r.jsx)(n.p,{children:"This guide covers advanced testing configuration for Merobox, including custom\ntesting utilities, performance testing, and integration with popular testing\nframeworks."}),"\n",(0,r.jsx)(n.h2,{id:"advanced-testing-configuration",children:"Advanced Testing Configuration"}),"\n",(0,r.jsx)(n.p,{children:"Configure Merobox for complex testing scenarios:"}),"\n",(0,r.jsx)(n.h3,{id:"basic-testing-setup",children:"Basic Testing Setup"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# conftest.py\nimport pytest\nfrom merobox.testing import cluster, workflow\n\n@pytest.fixture(scope="session")\ndef production_cluster():\n    """Production-like cluster for integration tests."""\n    with cluster(\n        count=3,\n        prefix="prod",\n        image="ghcr.io/calimero-network/merod:latest",\n        chain_id="mainnet-1",\n        wait_for_ready=True\n    ) as test_env:\n        yield test_env\n\n@pytest.fixture(scope="function")\ndef test_workflow():\n    """Run a complex workflow for each test."""\n    with workflow(\n        "workflows/test-setup.yml",\n        prefix="test",\n        scope="function"\n    ) as env:\n        yield env\n'})}),"\n",(0,r.jsx)(n.h3,{id:"advanced-cluster-configuration",children:"Advanced Cluster Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Advanced cluster setup\n@pytest.fixture(scope="session")\ndef advanced_cluster():\n    """Advanced cluster with custom configuration."""\n    with cluster(\n        count=5,\n        prefix="advanced",\n        image="ghcr.io/calimero-network/merod:edge",\n        chain_id="testnet-1",\n        wait_for_ready=True,\n        resources={\n            "memory": "2G",\n            "cpus": "1.0"\n        },\n        environment={\n            "RUST_LOG": "debug",\n            "CALIMERO_TEST_MODE": "true"\n        },\n        networks=["calimero-test", "calimero-internal"],\n        volumes={\n            "test-data": "/calimero/test-data"\n        }\n    ) as test_env:\n        yield test_env\n'})}),"\n",(0,r.jsx)(n.h3,{id:"multi-environment-testing",children:"Multi-Environment Testing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Multi-environment testing\n@pytest.fixture(params=["development", "staging", "production"])\ndef environment_cluster(request):\n    """Test against different environments."""\n    env_config = {\n        "development": {\n            "image": "ghcr.io/calimero-network/merod:dev",\n            "count": 2,\n            "resources": {"memory": "1G", "cpus": "0.5"}\n        },\n        "staging": {\n            "image": "ghcr.io/calimero-network/merod:staging",\n            "count": 3,\n            "resources": {"memory": "2G", "cpus": "1.0"}\n        },\n        "production": {\n            "image": "ghcr.io/calimero-network/merod:latest",\n            "count": 5,\n            "resources": {"memory": "4G", "cpus": "2.0"}\n        }\n    }\n\n    config = env_config[request.param]\n    with cluster(\n        count=config["count"],\n        prefix=request.param,\n        image=config["image"],\n        resources=config["resources"],\n        wait_for_ready=True\n    ) as test_env:\n        test_env["environment"] = request.param\n        yield test_env\n'})}),"\n",(0,r.jsx)(n.h2,{id:"custom-test-utilities",children:"Custom Test Utilities"}),"\n",(0,r.jsx)(n.p,{children:"Create custom testing utilities for your specific needs:"}),"\n",(0,r.jsx)(n.h3,{id:"basic-test-utilities",children:"Basic Test Utilities"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# test_utils.py\nfrom merobox.testing import cluster\nfrom merobox.commands.utils import get_node_rpc_url\nimport time\nimport requests\n\nclass TestEnvironment:\n    def __init__(self, node_count=2):\n        self.node_count = node_count\n        self.cluster = None\n\n    def __enter__(self):\n        self.cluster = cluster(count=self.node_count)\n        return self.cluster.__enter__()\n\n    def __exit__(self, *args):\n        if self.cluster:\n            self.cluster.__exit__(*args)\n\n    def get_node_endpoint(self, node_name):\n        """Get the RPC endpoint for a specific node."""\n        return get_node_rpc_url(node_name, self.cluster["manager"])\n\n    def wait_for_node_ready(self, node_name, timeout=60):\n        """Wait for a node to be ready."""\n        endpoint = self.get_node_endpoint(node_name)\n        start_time = time.time()\n\n        while time.time() - start_time < timeout:\n            try:\n                response = requests.get(f"{endpoint}/health", timeout=5)\n                if response.status_code == 200:\n                    return True\n            except requests.RequestException:\n                pass\n            time.sleep(1)\n\n        raise TimeoutError(f"Node {node_name} not ready after {timeout} seconds")\n\n    def get_all_endpoints(self):\n        """Get all node endpoints."""\n        endpoints = {}\n        for i in range(1, self.node_count + 1):\n            node_name = f"calimero-node-{i}"\n            endpoints[node_name] = self.get_node_endpoint(node_name)\n        return endpoints\n'})}),"\n",(0,r.jsx)(n.h3,{id:"advanced-test-utilities",children:"Advanced Test Utilities"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# advanced_test_utils.py\nimport asyncio\nimport aiohttp\nfrom merobox.testing import cluster\nfrom merobox.commands.utils import get_node_rpc_url\nimport json\nimport logging\n\nclass AsyncTestEnvironment:\n    def __init__(self, node_count=2):\n        self.node_count = node_count\n        self.cluster = None\n        self.session = None\n\n    async def __aenter__(self):\n        self.cluster = cluster(count=self.node_count)\n        self.cluster.__enter__()\n\n        # Create async HTTP session\n        self.session = aiohttp.ClientSession()\n        return self\n\n    async def __aexit__(self, *args):\n        if self.session:\n            await self.session.close()\n        if self.cluster:\n            self.cluster.__exit__(*args)\n\n    async def get_node_endpoint(self, node_name):\n        """Get the RPC endpoint for a specific node."""\n        return get_node_rpc_url(node_name, self.cluster["manager"])\n\n    async def call_node_method(self, node_name, method, params=None):\n        """Call a method on a specific node."""\n        endpoint = await self.get_node_endpoint(node_name)\n        url = f"{endpoint}/rpc"\n\n        payload = {\n            "jsonrpc": "2.0",\n            "method": method,\n            "params": params or [],\n            "id": 1\n        }\n\n        async with self.session.post(url, json=payload) as response:\n            return await response.json()\n\n    async def wait_for_consensus(self, timeout=60):\n        """Wait for all nodes to reach consensus."""\n        start_time = asyncio.get_event_loop().time()\n\n        while asyncio.get_event_loop().time() - start_time < timeout:\n            try:\n                # Check consensus on all nodes\n                tasks = []\n                for i in range(1, self.node_count + 1):\n                    node_name = f"calimero-node-{i}"\n                    task = self.call_node_method(node_name, "get_consensus_status")\n                    tasks.append(task)\n\n                results = await asyncio.gather(*tasks, return_exceptions=True)\n\n                # Check if all nodes have consensus\n                if all(not isinstance(result, Exception) and result.get("result", {}).get("consensus") for result in results):\n                    return True\n\n            except Exception as e:\n                logging.warning(f"Error checking consensus: {e}")\n\n            await asyncio.sleep(1)\n\n        raise TimeoutError(f"Consensus not reached after {timeout} seconds")\n\n    async def get_cluster_health(self):\n        """Get health status of all nodes."""\n        tasks = []\n        for i in range(1, self.node_count + 1):\n            node_name = f"calimero-node-{i}"\n            endpoint = await self.get_node_endpoint(node_name)\n            task = self.session.get(f"{endpoint}/health")\n            tasks.append((node_name, task))\n\n        health_status = {}\n        for node_name, task in tasks:\n            try:\n                response = await task\n                health_status[node_name] = await response.json()\n            except Exception as e:\n                health_status[node_name] = {"error": str(e)}\n\n        return health_status\n'})}),"\n",(0,r.jsx)(n.h3,{id:"test-data-management",children:"Test Data Management"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# test_data_utils.py\nimport json\nimport tempfile\nimport os\nfrom pathlib import Path\n\nclass TestDataManager:\n    def __init__(self, base_dir=None):\n        self.base_dir = base_dir or tempfile.mkdtemp()\n        self.test_data = {}\n\n    def create_test_application(self, name, version="1.0.0"):\n        """Create a test WASM application."""\n        app_data = {\n            "name": name,\n            "version": version,\n            "metadata": {\n                "description": f"Test application {name}",\n                "author": "test",\n                "created_at": "2024-01-01T00:00:00Z"\n            }\n        }\n\n        app_file = Path(self.base_dir) / f"{name}.wasm"\n        app_file.write_bytes(b"fake wasm content")\n\n        self.test_data[name] = {\n            "file": str(app_file),\n            "data": app_data\n        }\n\n        return str(app_file)\n\n    def create_test_workflow(self, name, steps):\n        """Create a test workflow file."""\n        workflow_data = {\n            "name": f"Test Workflow {name}",\n            "description": f"Test workflow for {name}",\n            "steps": steps\n        }\n\n        workflow_file = Path(self.base_dir) / f"{name}_workflow.yml"\n        workflow_file.write_text(json.dumps(workflow_data, indent=2))\n\n        return str(workflow_file)\n\n    def cleanup(self):\n        """Clean up test data."""\n        import shutil\n        if os.path.exists(self.base_dir):\n            shutil.rmtree(self.base_dir)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"performance-testing",children:"Performance Testing"}),"\n",(0,r.jsx)(n.p,{children:"Configure Merobox for performance testing:"}),"\n",(0,r.jsx)(n.h3,{id:"basic-performance-test",children:"Basic Performance Test"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# performance-test.yml\ndescription: Performance testing configuration\nname: Performance Test\n\n# High-performance node configuration\nnodes:\n  count: 5\n  image: ghcr.io/calimero-network/merod:edge\n  resources:\n    memory: '2G'\n    cpus: '1.0'\n  environment:\n    RUST_LOG: 'info'\n    CALIMERO_PERF_MODE: 'true'\n\n# Performance monitoring\nmonitoring:\n  enabled: true\n  metrics:\n    - cpu_usage\n    - memory_usage\n    - network_io\n    - disk_io\n  interval: 5\n\nsteps:\n  - name: Performance Test\n    type: repeat\n    count: 1000\n    parallel: 10\n    steps:\n      - name: Load Test\n        type: call\n        node: calimero-node-{{iteration % 5 + 1}}\n        method: load_test\n        args:\n          load: '{{iteration}}'\n"})}),"\n",(0,r.jsx)(n.h3,{id:"advanced-performance-testing",children:"Advanced Performance Testing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# performance_test.py\nimport asyncio\nimport aiohttp\nimport time\nimport statistics\nfrom merobox.testing import cluster\n\nclass PerformanceTest:\n    def __init__(self, node_count=5, test_duration=300):\n        self.node_count = node_count\n        self.test_duration = test_duration\n        self.results = []\n\n    async def run_load_test(self, cluster_env):\n        """Run load test against cluster."""\n        session = aiohttp.ClientSession()\n\n        try:\n            # Get all node endpoints\n            endpoints = []\n            for i in range(1, self.node_count + 1):\n                node_name = f"calimero-node-{i}"\n                endpoint = get_node_rpc_url(node_name, cluster_env["manager"])\n                endpoints.append(endpoint)\n\n            # Run load test\n            start_time = time.time()\n            tasks = []\n\n            while time.time() - start_time < self.test_duration:\n                # Create load test tasks\n                for endpoint in endpoints:\n                    task = self._make_request(session, endpoint)\n                    tasks.append(task)\n\n                # Wait for batch to complete\n                batch_results = await asyncio.gather(*tasks, return_exceptions=True)\n                self.results.extend([r for r in batch_results if not isinstance(r, Exception)])\n\n                tasks = []\n                await asyncio.sleep(0.1)  # Small delay between batches\n\n        finally:\n            await session.close()\n\n    async def _make_request(self, session, endpoint):\n        """Make a single request to test performance."""\n        start_time = time.time()\n\n        try:\n            async with session.post(f"{endpoint}/rpc", json={\n                "jsonrpc": "2.0",\n                "method": "get_status",\n                "params": [],\n                "id": 1\n            }) as response:\n                await response.json()\n                return time.time() - start_time\n        except Exception as e:\n            return None\n\n    def analyze_results(self):\n        """Analyze performance test results."""\n        if not self.results:\n            return {"error": "No results to analyze"}\n\n        response_times = [r for r in self.results if r is not None]\n\n        return {\n            "total_requests": len(self.results),\n            "successful_requests": len(response_times),\n            "failed_requests": len(self.results) - len(response_times),\n            "average_response_time": statistics.mean(response_times),\n            "median_response_time": statistics.median(response_times),\n            "p95_response_time": self._percentile(response_times, 95),\n            "p99_response_time": self._percentile(response_times, 99),\n            "requests_per_second": len(response_times) / self.test_duration\n        }\n\n    def _percentile(self, data, percentile):\n        """Calculate percentile of data."""\n        sorted_data = sorted(data)\n        index = int(len(sorted_data) * percentile / 100)\n        return sorted_data[min(index, len(sorted_data) - 1)]\n\n# Usage\nasync def test_performance():\n    with cluster(count=5, prefix="perf") as cluster_env:\n        perf_test = PerformanceTest(node_count=5, test_duration=60)\n        await perf_test.run_load_test(cluster_env)\n        results = perf_test.analyze_results()\n        print(f"Performance results: {results}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"stress-testing",children:"Stress Testing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# stress_test.py\nimport asyncio\nimport random\nfrom merobox.testing import cluster\n\nclass StressTest:\n    def __init__(self, node_count=3, max_concurrent=100):\n        self.node_count = node_count\n        self.max_concurrent = max_concurrent\n        self.semaphore = asyncio.Semaphore(max_concurrent)\n\n    async def run_stress_test(self, cluster_env):\n        """Run stress test with increasing load."""\n        tasks = []\n\n        # Gradually increase load\n        for load_level in range(10, self.max_concurrent + 1, 10):\n            print(f"Testing with {load_level} concurrent requests...")\n\n            # Create tasks for this load level\n            level_tasks = []\n            for _ in range(load_level):\n                task = self._stress_request(cluster_env)\n                level_tasks.append(task)\n\n            # Run tasks concurrently\n            results = await asyncio.gather(*level_tasks, return_exceptions=True)\n\n            # Analyze results\n            success_count = sum(1 for r in results if not isinstance(r, Exception))\n            print(f"Load level {load_level}: {success_count}/{load_level} successful")\n\n            # Wait before next load level\n            await asyncio.sleep(5)\n\n    async def _stress_request(self, cluster_env):\n        """Make a stress test request."""\n        async with self.semaphore:\n            # Randomly select a node\n            node_id = random.randint(1, self.node_count)\n            node_name = f"calimero-node-{node_id}"\n\n            # Make request with random delay\n            await asyncio.sleep(random.uniform(0, 0.1))\n\n            # Simulate request\n            try:\n                # Your actual request logic here\n                return "success"\n            except Exception as e:\n                return e\n'})}),"\n",(0,r.jsx)(n.h2,{id:"integration-with-testing-frameworks",children:"Integration with Testing Frameworks"}),"\n",(0,r.jsx)(n.h3,{id:"pytest-integration",children:"Pytest Integration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# conftest.py\nimport pytest\nfrom merobox.testing import pytest_cluster, pytest_workflow\n\n# Pytest fixtures\nmerobox_cluster = pytest_cluster(count=3, scope="session")\nmerobox_workflow = pytest_workflow("workflows/test.yml", scope="function")\n\n# Custom fixtures\n@pytest.fixture\ndef test_data():\n    """Provide test data for tests."""\n    return {\n        "test_app": "test-application.wasm",\n        "test_config": {"setting": "value"},\n        "test_params": ["param1", "param2"]\n    }\n\n@pytest.fixture\ndef mock_external_service():\n    """Mock external service for testing."""\n    # Your mock setup here\n    pass\n'})}),"\n",(0,r.jsx)(n.h3,{id:"test-cases",children:"Test Cases"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# test_integration.py\nimport pytest\nfrom merobox.testing import cluster\n\nclass TestCalimeroIntegration:\n    def test_basic_cluster_operation(self, merobox_cluster):\n        """Test basic cluster operations."""\n        endpoints = merobox_cluster["endpoints"]\n        assert len(endpoints) == 3\n\n        # Test each node\n        for node_name, endpoint in endpoints.items():\n            assert endpoint is not None\n            assert "calimero-node" in node_name\n\n    def test_workflow_execution(self, merobox_workflow):\n        """Test workflow execution."""\n        workflow_result = merobox_workflow["workflow_result"]\n        assert workflow_result is True\n\n        # Check workflow outputs\n        assert "node_endpoints" in merobox_workflow\n        assert len(merobox_workflow["node_endpoints"]) > 0\n\n    def test_application_installation(self, merobox_cluster, test_data):\n        """Test application installation."""\n        # Install test application\n        # Your installation logic here\n        pass\n\n    def test_performance_requirements(self, merobox_cluster):\n        """Test performance requirements."""\n        # Performance testing logic\n        pass\n\n    @pytest.mark.asyncio\n    async def test_async_operations(self, merobox_cluster):\n        """Test async operations."""\n        # Async testing logic\n        pass\n'})}),"\n",(0,r.jsx)(n.h3,{id:"continuous-integration",children:"Continuous Integration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# .github/workflows/test.yml\nname: Test\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.9'\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install merobox\n\n      - name: Run tests\n        run: |\n          pytest tests/ -v --cov=merobox\n\n      - name: Run performance tests\n        run: |\n          pytest tests/performance/ -v -m performance\n"})}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsx)(n.h3,{id:"test-organization",children:"Test Organization"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Modular Tests"}),": Organize tests into logical modules"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Test Data"}),": Use consistent test data management"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fixtures"}),": Create reusable test fixtures"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Documentation"}),": Document test cases and their purpose"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"performance-testing-1",children:"Performance Testing"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Baseline Metrics"}),": Establish baseline performance metrics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Load Patterns"}),": Test various load patterns and scenarios"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resource Monitoring"}),": Monitor resource usage during tests"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Regression Testing"}),": Detect performance regressions"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"error-handling",children:"Error Handling"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Test Failures"}),": Handle test failures gracefully"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cleanup"}),": Ensure proper cleanup after tests"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Debugging"}),": Provide good debugging information"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Retry Logic"}),": Implement retry logic for flaky tests"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"Now that you understand testing framework integration:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"./resource-management",children:"Resource Management"})," - Resource limits and monitoring"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"./security-configuration",children:"Security Configuration"})," - Security settings and\npolicies"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"./advanced-configuration",children:"Advanced Configuration"})," - Other advanced features"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},48854:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>a});var s=t(30758);const r={},o=s.createContext(r);function i(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);